<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="4DLangVGGT: 4D Language Visual Geometry Grounded Transformer">
  <meta name="keywords"
    content="4DLangVGGT, 3D Semantic Occupancy Prediction, Self-Supervised Learning, VGGT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>4DLangVGGT: 4D Language Visual Geometry Grounded Transformer</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">4DLangVGGT: 4D Language Visual Geometry Grounded Transformer</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=C9B5JKYAAAAJ&hl=en">Xianfeng Wu</a><sup>3,4</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=0bmTpcAAAAAJ&hl=zh-CN">Yajing Bai</a><sup>3</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=LhdBgMAAAAAJ&hl=en">Minghan Li</a><sup>2</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://xianzuwu.github.io/">Xianzu Wu</a><sup>5</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Xueqi_Zhao2">Xueqi Zhao</a><sup>1</sup>,&nbsp;</span><br>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=nEGuRZAAAAAJ&hl=zh-CN">Zhongyuan Lai</a><sup>1</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=D7jDk7gAAAAJ">Wenyu Liu</a><sup>3</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://xwcv.github.io/">Xinggang Wang</a><sup>3</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>State Key Laboratory of Precision Blasting, Jianghan University,&nbsp;</span><br>
              <span class="author-block"><sup>2</sup>Harvard AI and Robotics Lab, Harvard University,&nbsp;</span><br>
              <span class="author-block"><sup>3</sup>School of EIC, Huazhong University of Science and Technology,&nbsp;</span><br>
              <span class="author-block"><sup>4</sup>Department of Computing, The Hong Kong Polytechnic University,&nbsp;</span><br>
              <span class="author-block"><sup>5</sup>Department of Computer Science, Hong Kong Baptist University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://anonymous.4open.science/r/4dlangvggt" target="_blank"
                    class="external-link button is-normal is-rounded is-light">
                    <span class="icon">
                      <i class="fas fa-globe"></i>
                    </span>
                    <span>Project Page</span>
                  </a>
                </span>&nbsp;&nbsp;&nbsp;&nbsp;
                <span class="link-block">
                  <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-light">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>&nbsp;&nbsp;&nbsp;&nbsp;
                <span class="link-block">
                  <a href="https://github.com/hustvl/4DLangVGGT/" target="_blank"
                    class="external-link button is-normal is-rounded is-light">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications.
            </p>
            <p>
              To address these limitations, we propose <strong>4DLangVGGT</strong>, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity.
            </p>
            <p>
              Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding.
            </p>
            <p>
              Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to <strong>2%</strong> gains under per-scene training and <strong>1%</strong> improvements under multi-scene training.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <video poster="" id="vis1" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/glb_video.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="vis3" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/glb_video_large.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="vis4" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/glb_video_2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item">
            <video poster="" id="vis2" autoplay controls muted loop playsinline height="100%">
              <source src="static/videos/glb_video_2large.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Framework</h2>
      <div class="content has-text-justified">
        The framework integrates a geometry encoder, a semantic bridging decoder, 
        and a multi-objective training strategy to achieve language-aware 4D fields with geometric fidelity and semantic alignment.
      </div>
      <img src="static/images/framework.png">
      <div class="content has-text-justified">
        Input video frames are processed by StreamVGGT to obtain geometry tokens. The Semantic Bridging Decoder 
        predicts both RGB reconstructions and semantic embeddings, while the geometry decoder estimates depth maps 
        and camera poses. Inverse-projection lifts them into a 3D point cloud, onto which the predicted RGB and semantics 
        are colorized, yielding 3D frames and 3D semantic maps.
      </div>
      <img src="static/images/inference.png">
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        4DLangVGGT evaluate two training regimes to examine both cross-scene applicability and per-scene perfor-
mance. The first regime trains a single model on multiple videos and applies this shared model
for inference across different scenes (“multi-video single model”). The second regime adopts the
per-scene protocol used in 4DLangSplat, i.e., training one model per scene. This per-scene setting is
included to align with existing Gaussian splatting methods and to provide a fair comparison of our
method’s performance.
      </div>
      <img src="static/images/main result.png">
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Visulizations</h2>
      <div class="content has-text-justified">
        We comparison of time-agnostic query masks. The results demonstrate that our method
        consistently extracts accurate object masks in both intact and fragmented cookie scenarios, whereas
        4DLangSplat exhibits degraded performance when handling fragmented cases.
      </div>
      <img src="static/images/vis.png">
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">BibTeX</h2>
      <pre><code>@inproceedings{4DLangVGGT,title = {4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer},
        author = {Xianfeng Wu and Yajing Bai and Minghan Li and Xianzu Wu and Xueqi Zhao and Zhongyuan Lai and Wenyu Liu and Xinggang Wang},
        year = 2025,
        booktitle = {ArXiv}}
</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        Website built upon <a href="https://github.com/nerfies/nerfies.github.io">&nbsp;Nerfies</a>.
        Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">&nbsp;CC BY-SA 4.0
          License</a>.
      </div>
    </div>
  </footer>

</body>

</html>
